{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0e70e3-872e-4042-8e88-31f965b42587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05500ed-fc42-4abe-a894-dafa2ab949da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd82ab7-9ac8-4e7f-8b80-0f7b387d4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c6607-a93d-40b1-8f60-3576dc41f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", input_shape=IMAGE_SHAPE+(3,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f230cb5-d89b-498a-b743-f1bd0d133a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIL.Image.open('C:/Users/ys726/Desktop/DL/transfer_learning/goldfish.jpg')\n",
    "gold_fish = PIL.Image.open('./datasets/goldfish.jpg').resize(IMAGE_SHAPE)\n",
    "gold_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7ef9e-1f03-4312-bdaf-641f1a478d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_fish = np.array(gold_fish)/225\n",
    "gold_fish.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087fa982-821c-4211-81f1-e2bc64a46f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gold_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206eeb4-15cc-4400-bb4c-81817814d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We are adding one more dimenstion. the reason we are doing this because \n",
    "   when we do prediction so prediction accept multiple image as an input'''\n",
    "gold_fish[np.newaxis, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b63895-d457-4ea0-b87c-23e0793e99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(gold_fish[np.newaxis, ...])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b21ea-65e8-46b9-aefc-880b14ea1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784aea10-a44e-4b05-9a5f-44e4abf12e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_index = np.argmax(result)\n",
    "predicted_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729cfcc-68aa-4d56-a33d-3890602a9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = []\n",
    "with open(\"./datasets/ImageNetLabels.txt\", \"r\") as f:\n",
    "    image_labels = f.read().splitlines()\n",
    "\n",
    "image_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab820eec-91b4-4b69-9846-cd0032139359",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels[predicted_label_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045392c-7ee7-44dd-99b6-9520df9aece2",
   "metadata": {},
   "source": [
    "## Use this pretrained model for our flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffbf479-927b-4acc-a287-50fa401cc75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/example_images/flower_photos.tgz\n",
      "228813984/228813984 [==============================] - 39s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" \n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=url, cache_dir='.', untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa21e44b-aff6-4f36-8e97-fcac47760524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('datasets/flower_photos')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1f49dc-12c2-4674-9460-046dba8f6f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3670"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae478e39-db6f-45be-a5ac-051574b80644",
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "roses[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efd9b3-14f6-4d1b-8664-be7c4ab0fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(roses[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962ba7d-9f6a-4e64-bf22-3e330681eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips = list(data_dir.glob('tulips/*'))\n",
    "tulips[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef11ed-64af-4f67-b557-5d982d98bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(tulips[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322106c4-37b2-4c28-8ba5-19ae4199d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_image_dict = {\n",
    "    'roses' : list(data_dir.glob('roses/*')),\n",
    "    'tulips' : list(data_dir.glob('tulips/*')),\n",
    "    'dandelion' : list(data_dir.glob('dandelion/*')),\n",
    "    'sunflowers' : list(data_dir.glob('sunflowers/*')),\n",
    "    'daisy' : list(data_dir.glob('daisy/*')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a7e76b-b8f8-4e00-8f6f-45c32ade35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_labels_dict = {\n",
    "    'roses':0,\n",
    "    'daisy':1,\n",
    "    'dandelion':2,\n",
    "    'sunflowers':3,\n",
    "    'tulips':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48eea73d-cc09-49c6-aba8-419ed9cb1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset X contains flower images (3D numpy array) and y contains flower name (flower number)\n",
    "X, y = [], []\n",
    "for flower_name, images in flowers_image_dict.items():\n",
    "    for image in images:\n",
    "        img = cv2.imread(str(image))\n",
    "        resized_image = cv2.resize(img,IMAGE_SHAPE)\n",
    "        X.append(resized_image)\n",
    "        y.append(flowers_labels_dict[flower_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a6e2d1-dcde-4183-829a-739aa1dcceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list into numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27936625-b29a-411b-b2a6-b86709dc8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf071be3-656a-4e59-b067-4b6c2a3150cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752\n",
      "918\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de7ef7a-9130-4b16-b2dd-d12da5f1241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train/255\n",
    "X_test_scaled = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b324df49-51fd-4481-8db7-d7be10716e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2752, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'input layer size = 150528'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_scaled.shape)\n",
    "f\"input layer size = {224*224*3}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1aa79-b991-413f-8c17-dc72e85184a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e3f93-01b2-4695-8c84-0afd6e3555bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b9c74-715b-4d71-a41c-324ac1e4839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lry to use our classifier to pridict this model & classifier is the pretrained model\n",
    "predicted = classifier.predict(np.array([X[0], X[1], X[2]]))\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc641161-ca05-44db-9d71-c7ea3a79fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels[795], image_labels[880],image_labels[795]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba80b5-88bc-4d85-80ab-8002efc2257c",
   "metadata": {},
   "source": [
    "<b> So here we can not use readymate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5ac947e-fc42-4f86-ba57-b7b7df294344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3670, 224, 224, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76b99b92-9d8e-479a-8071-ca380444ddb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3670,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20e0f4ae-f69a-4940-b1a0-18869e43f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Now I am going to retrain this model.\n",
    "\n",
    "'''This will give us the same model as \n",
    "    previous one except the last layer'''\n",
    "\n",
    "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "pretrained_model_without_top_layer = hub.KerasLayer(\n",
    "    feature_extractor_model, input_shape=(224, 224, 3), trainable=False \n",
    "    ) # trainable=False means --> Don't train all those layes have fixed weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2146189b-282e-4725-9e52-59fb2b4e504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_of_flowers = 5 # (or output shape)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #Putting that readymate model\n",
    "    pretrained_model_without_top_layer,\n",
    "\n",
    "    # Dense Network\n",
    "    tf.keras.layers.Dense(num_of_flowers, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47a071-44cb-49db-abe7-ae13578fb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy']\n",
    "#               )\n",
    "\n",
    "# model.fit(X_train_scaled, y_train, epochs=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec1540-dec5-47de-aa13-1c03cb756e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd511bc6-ac7e-460b-ab7c-097c6c4def09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for applying the model\n",
    "def get_model():\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    #Putting that readymate model\n",
    "    pretrained_model_without_top_layer,\n",
    "\n",
    "    # Dense Network\n",
    "    tf.keras.layers.Dense(num_of_flowers, activation=\"softmax\")\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b2474-f9e8-4c50-88f6-9362dbfc5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on CPU\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    cpu_model = get_model()\n",
    "    cpu_model.fit(X_train_scaled, y_train, epochs=5)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"CPU Execution time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d93964-5c75-461e-819b-3c43a2beade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running on GPU\n",
    "# start_time = time.time()\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "#     gpu_model = get_model()\n",
    "#     gpu_model.fit(X_train_scaled, y_train, epochs=5)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"GPU Execution time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82494cfd-d339-40d2-a369-5726bbe76e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c497c-e7c8-47ec-89c2-2cac8185a24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3528b-039c-4270-bd93-58df9af33112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f8e92-5bdd-4dad-9ceb-ff0da955bcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
